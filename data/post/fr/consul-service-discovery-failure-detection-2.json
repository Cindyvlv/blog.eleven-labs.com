{"date":"2017-02-22T00:00:00.000Z","title":"Consul : Service Discovery et Failure Detection","excerpt":"# Introduction","readingTime":"8mn","authors":["vcomposieux"],"categories":[],"content":"# Introduction\n\nConsul est un outil développé en Go par la société HashiCorp et a vu le jour en 2013.\nConsul a plusieurs composants mais son objectif principal est de regrouper la connaissance des services d'une architecture (service discovery) et permet de s'assurer que les services contactés sont toujours disponibles en s'assurant que la santé de ces services est toujours bonne (via du health check).\n\nConcrètement, Consul va nous apporter un serveur DNS permettant de mettre à jour les adresses IP disponibles pour un service, en fonction de ceux qui sont en bonne santé. Ceci permet également de faire du load balancing bien que nous verrons qu'il ne permette pas pour le moment de préférer un service à un autre.\nIl offre également d'autres services tel que du stockage clé/valeur, nous l'utiliserons dans cet article afin que Docker Swarm y stocke ses valeurs.\n\nAfin de clarifier la suite de cet article, voici les ports utilisés par Consul :\n\n* `8300` (+ `8301` et `8302`) : Echanges via RPC,\n* `8400` : Echanges via RPC par le CLI,\n* `8500` : Utilisé pour l'API HTTP et l'interface web,\n* `8600` : Utilisé pour le serveur DNS.\n\nLa suite de cet article va se concentrer sur la partie service discovery et failure detection. Nous allons pour cela mettre en place un cluster Docker Swarm possédant l'architecture suivante :\n\n![](/imgs/posts/2017-02-22-consul-service-discovery-failure-detection-2/consul-archi.png)\n\nNous aurons donc 3 machines Docker :\n\n* Une machine avec `Consul` (Swarm Discovery),\n* Une machine étant notre \"`node 01`\" avec un service HTTP (Swarm),\n* Une machine étant notre \"`node 02`\" avec un service HTTP (Swarm).\n\nNous mettrons également sur nos deux nodes (cluster Docker Swarm) un container Docker pour Registrator, permettant de faciliter l'enregistrement de nos services Docker sur Consul.\n\nPour plus d'informations concernant `Registrator`, vous pouvez vous rendre sur : [https://gliderlabs.com/registrator/](https://gliderlabs.com/registrator/){:rel=\"nofollow noreferrer\"}\nCommençons à installer notre architecture !\n\n# Service discovery\n\n## Première machine : Consul (Swarm Discovery)\n\nNous allons commencer par créer la première machine : notre Consul.\n\nPour cela, tapez :\n\n```bash\n$ docker-machine create -d virtualbox consul\n```\n\nUne fois la machine prête, préparez votre environnement pour utiliser cette machine et lancez un container Consul :\n\n```bash\n$ eval $(docker-machine env consul)\n$ docker run -d \\\n    -p 8301:8301 \\\n    -p 8302:8302 \\\n    -p 8400:8400 \\\n    -p 8500:8500 \\\n    -p 53:8600/udp \\\n    consul\n```\n\nNous avons maintenant notre Consul prêt à recevoir nos services et nos prochaines machines membres de notre cluster Docker Swarm.\n\nVous pouvez d'ailleurs ouvrir l'interface web mise à disposition en obtenant l'ip de la machine Consul :\n\n```bash\n$ docker-machine ip consul\n<ip-obtenue>\n```\n\nPuis ouvrez dans votre navigateur l'URL : `http://<ip-obtenue>:8500`.\n\n## Deuxième machine : Node 01\n\nNous allons maintenant créer la machine correspondant au premier node de notre cluster Docker Swarm qui se verra également obtenir le rôle de master de notre cluster Swarm (il en faut bien un).\n\n```bash\n$ docker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-master \\\n    --swarm-discovery=\"consul://$(docker-machine ip consul):8500\" \\\n    --engine-opt=\"cluster-store=consul://$(docker-machine ip consul):8500\" \\\n    --engine-opt=\"cluster-advertise=eth1:2376\" swarm-node-01\n```\n\nComme vous le voyez, nous précisons l'option `--swarm-discovery`  avec l'IP de notre machine Consul et le port 8500 correspondant à l'API de Consul. Ainsi, Docker Swarm pourra utiliser l'API pour enregistrer les machines du cluster.\n\nNous allons maintenant configurer notre environnement pour utiliser cette machine et y installer dessus un container Registrator permettant d'auto-enregistrer les nouveaux services sur Consul.\n\nPour ce faire, tapez :\n\n```bash\n$ eval $(docker-machine env swarm-node-01)</pre>\n```\n\npuis :\n\n```bash\n$ docker run -d \\\n    --volume=/var/run/docker.sock:/tmp/docker.sock \\\n    gliderlabs/registrator \\\n    -ip $(docker-machine ip swarm-node-01) \\\n    consul://$(docker-machine ip consul):8500\n```\n\nVous remarquez que nous partageons le socket Docker sur la machine. Cette solution peut être [controversée](https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container.html mais dans le cas de cet article, passons là-dessus. Pour une architecture stable, nous préférerons enregistrer nous-même les services via l'API de Consul.\nL'option `-ip`  permet de préciser à Registrator l'IP sur laquelle nous voulons accéder aux services, à savoir l'IP de la machine et non pas l'IP interne du container Docker.\n\nNous sommes prêts à démarrer notre service HTTP. Celui-ci est une simple image Docker \"ekofr/http-ip\" qui lance une application HTTP écrite en Go et qui affiche \"hello, <ip>\" avec l'adresse IP du container courant.\n\nPour le besoin de cet article, nous allons également créer un réseau différent entre les deux machines afin d'identifier des adresses IP différentes pour les deux services.\n\nCréons donc un nouveau réseau pour notre node 01 :\n\n```bash\n$ docker network create \\\n    --subnet=172.18.0.0/16 network-node-01\n```\n\npuis utilisez ce réseau sur le container du service HTTP :\n\n```bash\n$ docker run -d \\\n    --net network-node-01 \\\n    -p 80:8080 \\\n    ekofr/http-ip\n```\n\nAvant d'exécuter les mêmes étapes pour créer notre node 02, assurons-nous d'avoir un service fonctionnel :\n\n```bash\n$ curl http://localhost:80\nhello from 172.18.0.X\n```\n\n## Troisième machine : Node 02\n\nNous allons donc (presque) répéter les étapes du node 01 en changeant quelques valeurs seulement. Créez la machine :\n\n```bash\n$ docker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-discovery=\"consul://$(docker-machine ip consul):8500\" \\\n    --engine-opt=\"cluster-store=consul://$(docker-machine ip consul):8500\" \\\n    --engine-opt=\"cluster-advertise=eth1:2376\" swarm-node-02\n```\n\nPréparez votre environnement pour utiliser cette machine node 02 et installez-y Registrator :\n\n```bash\n$ eval $(docker-machine env swarm-node-02)\n```\n\n```bash\n$ docker run -d \\\n    --volume=/var/run/docker.sock:/tmp/docker.sock \\\n    gliderlabs/registrator \\\n    -ip $(docker-machine ip swarm-node-02) \\\n    consul://$(docker-machine ip consul):8500\n```\n\nPuis créez un nouveau réseau et lancez le service HTTP avec ce réseau :\n\n```bash\n$ docker network create \\\n     --subnet=172.19.0.0/16 network-node-02\n```\n\n```bash\n$ docker run -d \\\n    --net network-node-02 \\\n    -p 80:8080 \\\n    ekofr/http-ip\n```bash\n\nNous voilà prêt à découvrir ce que nous apporte Consul.\n\n# Requêtes DNS\n\nVous pouvez en effet maintenant résoudre votre service `http-ip.service.consul`  en utilisant le serveur DNS apporté par Consul, vous devriez voir vos deux services enregistrés :\n\n```bash\n$ dig @$(docker-machine ip consul) http-ip.service.consul\n\n;; QUESTION SECTION:\n;http-ip.service.consul. IN A\n\n;; ANSWER SECTION:\nhttp-ip.service.consul. 0 IN A 192.168.99.100\nhttp-ip.service.consul. 0 IN A 192.168.99.102\n```\n\nAutrement dit, un load balancing sera fait sur un de ces deux services lorsque vous chercherez à joindre `http://http-ip.service.consul`.\nOui, mais qu'en est-il du côté de la répartition de cette charge ? Pouvons-nous définir une priorité et/ou poids ?\n\nMalheureusement, la réponse est non, pas pour le moment. Une issue a cependant été ouverte sur Github pour demander le support de celui-ci : [https://github.com/hashicorp/consul/issues/1088](https://github.com/hashicorp/consul/issues/1088){:rel=\"nofollow noreferrer\"}.\n\nEn effet, si nous regardons de plus près l'enregistrement DNS de type `SRV` , voici ce que nous obtenons :\n\n```bash\n$ dig @$(docker-machine ip consul) http-ip.service.consul SRV\n\n;; ANSWER SECTION:\nhttp-ip.service.consul. 0 IN SRV 1 1 80 c0a86366.addr.dc1.consul.\nhttp-ip.service.consul. 0 IN SRV 1 1 80 c0a86364.addr.dc1.consul.\n```\n\nComme vous pouvez le voir, la priorité et le poids sont tous les deux définis à 1, le load balancing sera donc équilibré entre tous les services.\n\nSi vous ajoutez l'IP de la machine Consul en tant que serveur DNS sur votre système d'exploitation, vous pourrez donc appeler votre service en HTTP et vous rendre compte plus facilement du load balancing :\n\n```bash\n$ curl http://http-ip.service.consul\nhello from 172.18.0.2\n\n$ curl http://http-ip.service.consul\nhello from 172.19.0.2\n```\n\nNous avons ici une IP correspondant à chaque service HTTP que nous avons enregistré.\n\n# Failure detection\n\nNous allons maintenant ajouter un Health Check à notre service afin de s'assurer que celui-ci peut être utilisé.\n\nNous allons donc commencer par retourner sur notre node 01 et supprimer le container `ekofr/http-ip`  afin de le recréer avec un Health Check :\n\n```bash\n$ eval $(docker-machine env swarm-node-01)\n```\n\n```bash\n$ docker kill \\\n$(docker ps -q --filter='ancestor=ekofr/http-ip')\n```\n\nRegistrator nous offre des variables d'environnement afin d'ajouter des Health Check de nos containers à Consul, vous pouvez consulter la liste de toutes les variables disponibles ici : [http://gliderlabs.com/registrator/latest/user/backends/#consul](http://gliderlabs.com/registrator/latest/user/backends/#consul){:rel=\"nofollow noreferrer\"}.\n\nL'idée est pour nous de vérifier que le port 80 répond correctement, nous allons donc ajouter un script exécutant simplement une requête curl. Pour ce faire :\n\n```bash\n$ docker run -d \\\n    --net network-node-01 -p 80:8080 \\\n    -e SERVICE_CHECK_SCRIPT=\"curl -s -f http://$(docker-machine ip swarm-node-01)\" \\\n    -e SERVICE_CHECK_INTERVAL=5s \\\n    -e SERVICE_CHECK_TIMEOUT=1s \\\n    ekofr/http-ip\n```\n\nVous pouvez faire de même sur le node 02 (en faisant attention à bien modifier les `node-01`  en `node-02` ) et vous devriez maintenant pouvoir visualiser ces checks sur l'interface Consul :\n\n![Consul Infrastructure Schema](/imgs/posts/2017-02-22-consul-service-discovery-failure/schema.png)\n\nDe la même façon, vous pouvez également utiliser l'API de Consul afin de vérifier la santé de vos services :\n\n```bash\n$ curl http://$(docker-machine ip consul):8500/v1/health/checks/http-ip\n[\n  {\n    \"Status\": \"passing\",\n    \"Output\": \"hello from 172.18.0.2\",\n    \"ServiceName\": \"http-ip\",\n  },\n  ...\n]\n```\n\n# Conclusion\n\nVous pouvez maintenant mettre en place Consul sur vos architectures afin de vous assurer que les services contactés sont bien disponibles mais surtout pouvoir identifier les éventuels problèmes qui peuvent survenir sur vos services.\nIl est donc important d'ajouter un maximum de checks sur les éléments pouvant rendre vos services indisponibles (vérifier que celui-ci peut bien être contacté, vérifier l'espace disque disponible sur la machine, etc ...).\n\nConsul est un outil qui s'intègre parfaitement dans vos architectures, grâce à son utilisation très simple et son API complète.\n"}